# Predict housing prices for records in the test set using the pruned tree
rt.pruned.pred <- predict(rt.pruned, newdata = test.df, type = "vector")
# Evaluate the model performance by computing the mean squared error (MSE) in the test set
mse <- mean((test.df$MEDV - rt.pruned.pred)^2)
# Return the first six predicted values and the mean squared error
list(predicted_values = head(rt.pruned.pred),
mean_squared_error = mse)
knitr::opts_chunk$set(warning=FALSE, message=FALSE)
library('rpart.plot')
# load the data
housing.df <- read.csv('BostonHousing.csv')
# remove the variable CAT..MEDV
housing.df <- housing.df[, -14]
# first six rows
head(housing.df)
# column names
names(housing.df)
# set the seed
set.seed(1)
# row numbers of the training set
train.index <- sample(c(1:dim(housing.df)[1]), dim(housing.df)[1]*0.6)
head(train.index)
# training set
train.df <- housing.df[train.index, ]
head(train.df)
# test set
test.df <- housing.df[-train.index, ]
head(test.df)
# regression tree with cp = 0.5
rt.shallow <- rpart(MEDV ~ ., data = train.df,method="anova",cp = 0.5)
# plot the tree
prp(rt.shallow, type = 1, extra = 1)
# predicted prices for records in the test set
rt.shallow.pred <- predict(rt.shallow, newdata = test.df, type = "vector")
# first six values
head(rt.shallow.pred)
# MSE in the test set
mean((test.df$MEDV - rt.shallow.pred)^2)
# regression tree with cp = 0.01
rt.shallow <- rpart(MEDV ~ ., data = train.df,method="anova",cp = 0.01)
# plot the tree
prp(rt.shallow, type = 1, extra = 1)
# predicted prices for records in the test set
rt.shallow.pred <- predict(rt.shallow, newdata = test.df, type = "vector")
# first six values
head(rt.shallow.pred)
# MSE in the test set
mean((test.df$MEDV - rt.shallow.pred)^2)
# Set the random seed for reproducibility
set.seed(1)
# Assuming you have the training data in train.df
# Fit a regression tree with 5-fold cross-validation
cv.rt <- rpart(MEDV ~ ., data = train.df, method = "anova", cp = 0.001, xval = 5)
# Display various complexity parameter values and their cross-validated errors
cv.rt$cptable
# Get the cp value that corresponds to the lowest cross-validated error
cv.rt$cptable[which.min(cv.rt$cptable[,"xerror"]),"CP"]
# Find the pruned tree using the best cp value
rt.pruned <- prune(cv.rt, cp = best_cp)
# Plot the pruned tree
prp(rt.pruned, type = 1, extra = 1)
# Predict housing prices for records in the test set using the pruned tree
rt.pruned.pred <- predict(rt.pruned, newdata = test.df, type = "vector")
# Evaluate the model performance by computing the mean squared error (MSE) in the test set
mse <- mean((test.df$MEDV - rt.pruned.pred)^2)
# Return the first six predicted values and the mean squared error
list(predicted_values = head(rt.pruned.pred),
mean_squared_error = mse)
knitr::opts_chunk$set(warning=FALSE, message=FALSE)
bank.df <- read.csv("UniversalBank.csv")
head(bank.df)
# drop ID and zip code columns.
bank.df <- bank.df[ , -c(1, 5)]
head(bank.df)
# convert numeric variables to categorical variables
bank.df$Education <- as.factor(bank.df$Education)
bank.df$Personal.Loan <- as.factor(bank.df$Personal.Loan)
str(bank.df)
# total number of rows
dim(bank.df)[1]
# size of the training set
dim(bank.df)[1]*0.6
# set seed
set.seed(1)
# row index of the training set
train.index <- sample(c(1:dim(bank.df)[1]), dim(bank.df)[1]*0.6)
head(train.index)
# training set
train.df <- bank.df[train.index, ]
head(train.df)
# test set
test.df <- bank.df[-train.index, ]
head(test.df)
library(adabag)
Sys.setenv(RGL_USE_NULL=TRUE)
library(adabag)
# Use the trained neural network to make predictions on the test set
nn.pred <- compute(nn, test.norm.df)
# Use the trained neural network to make predictions on the test set
nn.pred <- compute(nn, test.norm.df)
y <- seq(from=1, to=10, length.out = 5)
y
x_vector_named <- c(a=2, b=4, c=6, d=8)
x_vector_named[[b]]
x_vector_named[["b"]]
x_vector_named["b"]
knitr::opts_chunk$set(warning=FALSE, message=FALSE)
# load the data
loan.df <- read.csv("orig_svcg_2005_48mo.csv")
head(loan.df)
# create frac_unpaid
loan.df$frac_current_upb <- loan.df$current_upb/loan.df$orig_upb
head(loan.df$frac_current_upb)
# create msa: equal to TRUE if the mortgaged property is located in a MSA, and FALSE otherwise
loan.df$msa <- !is.na(loan.df$cd_msa)
head(loan.df$cd_msa)
head(loan.df$msa)
# create other_servicers: equals TRUE if the servicer name is in the category 'other servicers', and FALSE otherwise
loan.df$other_servicers<- (loan.df$servicer_name == "Other servicers")
head(loan.df$servicer_name)
head(loan.df$other_servicers)
# select outcome and potential predictors
df <-  loan.df[, c("frac_current_upb", "orig_upb", "fico", "mi_pct", "dti", "ltv", "int_rt","cnt_units", "cnt_borr",
"msa", "other_servicers", "flag_fthb", "prop_type","occpy_sts")]
head(df)
# column names
names(df)
knitr::opts_chunk$set(warning=FALSE, message=FALSE)
# load the data
loan.df <- read.csv("orig_svcg_2005_48mo.csv")
head(loan.df)
# create frac_unpaid
loan.df$frac_current_upb <- loan.df$current_upb/loan.df$orig_upb
head(loan.df$frac_current_upb)
# create msa: equal to TRUE if the mortgaged property is located in a MSA, and FALSE otherwise
loan.df$msa <- !is.na(loan.df$cd_msa)
head(loan.df$cd_msa)
head(loan.df$msa)
# create other_servicers: equals TRUE if the servicer name is in the category 'other servicers', and FALSE otherwise
loan.df$other_servicers<- (loan.df$servicer_name == "Other servicers")
head(loan.df$servicer_name)
head(loan.df$other_servicers)
# select outcome and potential predictors
df <-  loan.df[, c("frac_current_upb", "orig_upb", "fico", "mi_pct", "dti", "ltv", "int_rt","cnt_units", "cnt_borr",
"msa", "other_servicers", "flag_fthb", "prop_type","occpy_sts")]
head(df)
# column names
names(df)
# number of missing values in the first 10 rows of the original data frame
is.na(loan.df[1:10,])
sum(is.na(loan.df[1:10,]))
# dimension
dim(df)
# number of cells with missing values
sum(is.na(df))
# remove rows that have missing values in any variable
df <- na.omit(df)
# dimension
dim(df)
# number of cells with missing values
sum(is.na(df))
# data frame
is.data.frame(df)
# convert a data frame of predictors to a matrix
x <- model.matrix(frac_current_upb~.,df)[,-1]
# model.matrix creates dummy variables for character variables
head(x)
# matrix
is.matrix(x)
# outcome
y <- df$frac_current_upb
# vector
is.vector(y)
# row indexes of the training set
set.seed(1)
train.index <- sample(c(1:dim(x)[1]), dim(x)[1]*0.5)
head(train.index)
# predictors in the training set
head(x[train.index,])
# outcome in the training set
head(y[train.index])
# row indexes of the test set
test.index <- (-train.index)
head(test.index)
# predictors in the test set
head(x[test.index,])
# outcome in the test set
y.test <- y[test.index]
head(y.test)
# fit a lasso regression model
library(glmnet)
fit<- glmnet(x[train.index,],y[train.index],alpha=1)
# alpha=1 specifies a lasso regression model
# sequence of lambda values
fit$lambda
# dimension of lasso regression coefficients
# 19 coefficients (intercept plus 18 predictors) for each value of lambda
dim(coef(fit))
# plot coefficients on log of lambda values
plot(fit, xvar="lambda")
# return a small lambda value
lambda.small <- fit$lambda[70]
lambda.small
# lasso regression coefficients
coef.lambda.small <- predict(fit,s=lambda.small,type="coefficients")[1:19,]
# s: value of the penalty parameter lambda
# type=coefficients computes the coefficients at the requested lambda value
coef.lambda.small
# non-zero coefficient estimates
coef.lambda.small[coef.lambda.small!=0]
# make predictions for records in the test set
pred.lambda.small <- predict(fit,s=lambda.small,newx=x[test.index,])
head(pred.lambda.small)
# MSE in the test set
mean((y.test-pred.lambda.small)^2)
# return a large lambda value
lambda.large <- fit$lambda[1]
lambda.large
# lasso regression coefficients
coef.lambda.large <- predict(fit,s=lambda.large,type="coefficients")[1:19,]
# s: value of the penalty parameter lambda
# type=coefficients computes the coefficients at the requested lambda value
coef.lambda.large
# non-zero coefficient estimates
coef.lambda.large[coef.lambda.large!=0]
# make predictions for records in the test set
pred.lambda.large <- predict(fit,s=lambda.large,newx=x[test.index,])
head(pred.lambda.large)
# prediction is the mean
mean(y[train.index])
# MSE in the test set
mean((y.test-pred.lambda.large)^2)
# fit a lasso regression model with 10-fold cross-validation on the training set
set.seed(1)
cv.fit <- cv.glmnet(x[train.index,],y[train.index],alpha=1, type.measure="mse")
# alpha=1 specifies a lasso regression model
# type.measure="mse" specifies the criterion: cross-validated MSE
# nfold=10 performs 10-fold cross validation by default
# cross-validated MSE for each lambda
plot(cv.fit)
# lambda that corresponds to the lowest cross-validated MSE
lambda.best <- cv.fit$lambda.min
lambda.best
# vertical line on the graph
log(lambda.best)
# lasso regression coefficients
coef.lambda.best <- predict(cv.fit,s=lambda.best,type="coefficients")[1:19,]
coef.lambda.best
# non-zero coefficients
coef.lambda.best[coef.lambda.best!=0]
# make predictions for records the test set
pred.lambda.best <- predict(cv.fit,s=lambda.best,newx=x[test.index,])
head(pred.lambda.best)
# MSE in the test set
mean((y.test-pred.lambda.best)^2)
install.packages('glmnet')
install.packages("glmnet")
y <- list(a = 1:3, b = "a string", c = TRUE, d = list(-1, -5))
y
y[c("a", "d")]
knitr::opts_chunk$set(warning=FALSE, message=FALSE)
# load the data
df <- read.csv("TinyData.csv")
# create two outcome dummies
df$Like <- df$Preference=="like"
df$Dislike <- df$Preference=="dislike"
df
library(neuralnet)
set.seed(1)
nn1<- neuralnet(Like + Dislike ~ x2 + x1, data = df, linear.output = FALSE, hidden = 3)
# hidden: a vector of integers specifying the number of hidden neurons in each layer
# linear.output=TRUE for regression and linear.output=FALSE for classification
# display weights
nn1$weights
# plot the network
# rep="best": the repetition of the neural network with the smallest error will be plotted
plot(nn1, rep = "best")
# display the activation function
nn1$act.fct
# create a data frame
data.frame(df$x2, df$x1)
predict <- compute(nn1, data.frame(df$x2, df$x1))
predict$net.result
# predicted probabilities of like
predict$net.result[,1]
# predicted probabilities of dislike
predict$net.result[,2]
predicted.class <- ifelse(predict$net.result[,2]>predict$net.result[,1], "dislike", "like")
predicted.class
# predicted classes are character values
predicted.class
# actual classes are character values
df$Preference
library(caret)
confusionMatrix(as.factor(predicted.class), as.factor(df$Preference),positive="like")
toyota.df <- read.csv("ToyotaCorolla.csv")
head(toyota.df)
# Fuel_Type has 3 levels
toyota.df$Fuel_Type_CNG <- ifelse(toyota.df$Fuel_Type == "CNG",1,0)
toyota.df$Fuel_Type_Diesel <- ifelse(toyota.df$Fuel_Type == "Diesel",1,0)
set.seed(1)
# total number of rows
dim(toyota.df)[1]
# number of rows to select for the training set
0.6*dim(toyota.df)[1]
# row indexes of the training set
train.index <- sample(c(1:dim(toyota.df)[1]), 0.6*dim(toyota.df)[1])
head(train.index)
# training set
train.df <- toyota.df[train.index, ]
head(train.df)
# test set
test.df <- toyota.df[-train.index, ]
head(test.df)
# maximum price in the training set
max(train.df$Price)
# minimum price in the training set
min(train.df$Price)
# training set
head(train.df$Price)
# normalized price for the first record
(9250-min(train.df$Price))/(max(train.df$Price)-min(train.df$Price))
# estimate the transformation
library(caret)
norm.values <- preProcess(train.df, method="range")
# method="range": scale the data to the interval between zero and one
# normalize the numerical predictors and the outcome variable in the training set
train.norm.df <- predict(norm.values, train.df)
# Price, Age_08_04, KM, HP, Doors, Quarterly_Tax, Guarantee_Period are normalized
head(train.norm.df)
# normalize the numerical predictors and the outcome variable in the test set
test.norm.df <- predict(norm.values, test.df)
head(test.norm.df)
# set the seed for reproducing the result
set.seed(1)
nn2 <- neuralnet(Price ~ Age_08_04+KM+Fuel_Type_CNG+Fuel_Type_Diesel+HP+Automatic+Doors+Quarterly_Tax+Mfr_Guarantee
+Guarantee_Period+Airco+Automatic_airco+CD_Player+Powered_Windows+Sport_Model+Tow_Bar,
data = train.norm.df, linear.output = TRUE, hidden = 2)
# hidden: a vector of integers specifying the number of hidden neurons in each layer
# linear.output=TRUE for regression and linear.output=FALSE for classification
# rep="best": the repetition of the neural network with the smallest error will be plotted
plot(nn2, rep = "best")
predict.nn2 <- compute(nn2, test.norm.df)
# predicted prices (normalized)
head(predict.nn2$net.result)
mean((test.norm.df$Price-predict.nn2$net.result)^2)
install.packages(c("evaluate", "fansi", "foreign", "ggplot2", "htmltools", "KernSmooth", "knitr", "lattice", "markdown", "Matrix", "mgcv", "nlme", "plyr", "pROC", "RcppEigen", "spatial", "survival", "tinytex", "utf8", "vctrs", "vroom", "withr", "xfun", "XML"))
R.home()
install.packages("tidyverse")
## load necessary packages
library(tidyverse)
# read and write Stata DTA files
library(haven)
# this package contains extra geoms for ggplot2
library(ggrepel)
# geom_text_repel adds text directly to the plot
# load dta data
data <- read_dta("https://reifjulian.github.io/illinois-wellness-data/data/stata/claims.dta")
knitr::opts_chunk$set(warning=FALSE, message=FALSE)
## load necessary packages
library(tidyverse)
# read and write Stata DTA files
library(haven)
# this package contains extra geoms for ggplot2
library(ggrepel)
# geom_text_repel adds text directly to the plot
# load dta data
data <- read_dta("https://reifjulian.github.io/illinois-wellness-data/data/stata/claims.dta")
## 1.1 treatment group indicator
head(data$treat)
# 1 for employees in the treatment group, and 0 for employees in the control group
## 1.2 convert numeric values to categorical values
head(factor(data$treat, levels = c(0,1), labels = c("Control", "Treatment")))
# levels: a vector of the unique values that x might have taken
# labels: a character vector of labels for the levels
# 1.3 wellness program participation indicator
head(data$hra_c_yr1)
# NA for employees in the control group, 0 for non-participants in the treatment group, and 1 for participants in the treatment group
## 1.4 summary statistics by group
data %>%
mutate(treat = factor(treat, levels = c(0,1), labels = c("Control", "Treatment"))) %>%
group_by(treat) %>%
summarise(n_employees = n(), n_participants = sum(hra_c_yr1))
# n_participants: total number of participants
# n_employees: total number of employees
## 1.5 visualize the number of employees by the control and treatment group
data %>%
mutate(treat = factor(treat, levels = c(0,1), labels = c("Control", "Treatment"))) %>%
group_by(treat) %>%
summarise(n_employees = n(), n_participants = sum(hra_c_yr1)) %>%
ggplot() +
stat_summary(mapping = aes(x = treat, y = n_employees), geom = 'bar', fun = 'identity') +
geom_text_repel(aes(x = treat, y = n_employees, label = n_employees), nudge_y = 20)
# geom_text_repel adds text directly to the plot
# nudge_y: vertical adjustments to nudge the starting position of each text label
# treatment group indicator
head(data$treat)
# 0 for employees in the control group, and 1 for employees in the treatment group
# total spending from 2015 to 2016
m_1 <- lm(spend_0715_0716 ~ treat, data = data)
# regression result
summary(m_1)
# regression result without formula
coef(summary(m_1))
# intercept
coef(summary(m_1))['(Intercept)', 'Estimate']
# intercept + treatment
coef(summary(m_1))['(Intercept)', 'Estimate'] + coef(summary(m_1))['treat', 'Estimate']
# p-value
coef(summary(m_1))['treat', 'Pr(>|t|)']
## pre-treatment outcome variables from 2015 to 2016
# "spend_0715_0716": total spending
# "spendOff_0715_0716": office spending ($)
# "spendHosp_0715_0716": hospital spending ($)
# "spendRx_0715_0716": drug spending
# "nonzero_spend_0715_0716": nonzero medical spending
# treatment group indicator
head(data$treat)
# 0 for employees in the control group, and 1 for employees in the treatment group
## 3.1 no control
m_1_no_controls <- lm(spend_0816_0717 ~ treat, data = data)
# regression result
summary(m_1_no_controls)
# regression result without formula
coef(summary(m_1_no_controls))
# coefficient
coef(summary(m_1_no_controls))['treat', 'Estimate']
# standard error
coef(summary(m_1_no_controls))['treat', 'Std. Error']
## 3.2 with control
m_1_controls <- lm(spend_0816_0717 ~ treat + male + white + age37_49 + age50, data = data)
# male: sex (male/female)
# white: race (white/nonwhite)
# age37_49: middle age group (37-49/not 37-49)
# age50: oldest age group (50+/not 50+)
# regression result
summary(m_1_controls)
# regression result without formula
coef(summary(m_1_controls))
# coefficient
coef(summary(m_1_controls))['treat', 'Estimate']
# standard error
coef(summary(m_1_controls))['treat', 'Std. Error']
## outcome variables from 2016 to 2017
# "spend_0816_0717": total spending
# "spendOff_0816_0717": office spending
# "spendHosp_0816_0717": hospital spending
# "spendRx_0816_0717": drug spending
# "nonzero_spend_0816_0717": nonzero medical spending
# participant indicator
head(data$hra_c_yr1)
# NA for employees in the control group, 0 for non-participants in the treatment group, and 1 for participants in the treatment group
## 4.1 no control
n_1_no_controls <- lm(spend_0816_0717 ~ hra_c_yr1, data = data)
# regression result
summary(n_1_no_controls)
# regression result without formula
coef(summary(n_1_no_controls))
# coefficient
coef(summary(n_1_no_controls))['hra_c_yr1', 'Estimate']
# standard error
coef(summary(n_1_no_controls))['hra_c_yr1', 'Std. Error']
## 4.2 with control
n_1_controls <- lm(spend_0816_0717 ~ hra_c_yr1 + male + white + age37_49 + age50, data = data)
# male: sex (male/female)
# white: race (white/nonwhite)
# age37_49: middle age group (37-49/not 37-49)
# age50: oldest age group (50+/not 50+)
# regression result
summary(n_1_controls)
# regression result without formula
coef(summary(n_1_controls))
# coefficient
coef(summary(n_1_controls))['hra_c_yr1', 'Estimate']
# standard error
coef(summary(n_1_controls))['hra_c_yr1', 'Std. Error']
## outcome variables from 2016 to 2017
# "spend_0816_0717": total spending
# "spendOff_0816_0717": office spending ($)
# "spendHosp_0816_0717": hospital spending ($)
# "spendRx_0816_0717": drug spending
# "nonzero_spend_0816_0717": nonzero medical spending
View(m_1)
View(data)
# Load required libraries
library(dplyr)
# Load required libraries
library(dplyr)
library(caret)
library(ggplot2)
library(rpart)
# Load the data files
predict_data <- read.csv("predict_property_data.csv")
getwd()
getwd()
setwd('/Users/junmingyi/Library/CloudStorage/OneDrive-温州肯恩大学/Documents/2023 Fall/FIN 550/project/data')
predict_data <- read.csv("predict_property_data.csv")
historic_data <- read.csv("historic_property_data.csv")
codebook <- read.csv("codebook.csv")
# Load the data files
predict_data <- read.csv("https://github.com/Junmingyi0112/https---github.com-Junmingyi0112-FIN550-FA23-G7/blob/master/predict_property_data.csv")
# Load the data files
predict_data <- read.csv("https://github.com/Junmingyi0112/https---github.com-Junmingyi0112-FIN550-FA23-G7/blob/master/predict_property_data.csv")
historic_data <- read.csv("https://github.com/Junmingyi0112/https---github.com-Junmingyi0112-FIN550-FA23-G7/blob/master/historic_property_data.csv")
# Load the data files
historic_data <- read.csv("https://github.com/Junmingyi0112/https---github.com-Junmingyi0112-FIN550-FA23-G7/blob/master/historic_property_data.csv")
# Load the data files
historic_data <- read.csv("https://github.com/Junmingyi0112/https---github.com-Junmingyi0112-FIN550-FA23-G7/blob/master/historic_property_data.csv")
predict_data <- read.csv("https://github.com/Junmingyi0112/https---github.com-Junmingyi0112-FIN550-FA23-G7/blob/master/predict_property_data.csv")
# Load the data files
historic_data <- read.csv("https://github.com/Junmingyi0112/FIN550-FA23-G7/blob/main/historic_property_data.csv")
View(historic_data)
